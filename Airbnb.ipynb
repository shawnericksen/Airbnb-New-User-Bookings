{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74756a7",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "Group: 9\n",
    "Group Members: Shawn Ericksen (ericksen@uark.edu), Garret Fulghum (gmfulghu@uark.edu), Wesley Parker (wgparker@uark.edu)\n",
    "\n",
    "This practice project focuses on the Airbnb New User Bookings dataset. This can be accessed from: https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d69a47",
   "metadata": {},
   "source": [
    "## Kaggle Performance Info\n",
    "Kaggle's scoring for this competition utilizes a Normalized Discounted Cumulative Gain (NDCG) scoring, where up to 5 guesses of destination county (ordered by confidence) are submitted per entry in the test data. A score of 1.0 reflects the first guess being correct, and less points for other scenarios (0.63 for the second guess being correct and so on).\n",
    "\n",
    "Q1/Q2/Q3 Kaggle scores: 0.85226 / 0.86555 / 0.86661\n",
    "\n",
    "Notes:\n",
    "\n",
    "The baseline, which is to always guess NDF-US-OTHER-FR-IT, recieves a NDCG score of 0.8219 (78th Percentile) on the Public Leaderboard. The NDCG score of the dummy model on the training data is 0.80676.\n",
    "\n",
    "As for scikit's accuracy score, always guessing NDF results in a score of 0.58347 against the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b246ff",
   "metadata": {},
   "source": [
    "### Usage\n",
    "Running the second code cell will prompt the user read data from CSV or HDF5 (setting \\_\\_fast_hdf__ will skip the prompt and use HDF5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b369d",
   "metadata": {},
   "source": [
    "### Imports and reading dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f11fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from sklearnex import patch_sklearn \n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline # , Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.metrics import ndcg_score # , classification_report\n",
    "\n",
    "# from sklearn.experimental import enable_halving_search_cv\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "from joblib import parallel_backend\n",
    "# from ray.util.joblib import register_ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "__no_prompt__ = False # If True, skips user input (defaults to HDF)\n",
    "__no_hdf__ = False # If True, only reads from CSV\n",
    "\n",
    "data_folder = Path(\"airbnb-recruiting-new-user-bookings/\") # Replace if using Kaggle Notebook\n",
    "hdf_path = data_folder / \"data.h5\"\n",
    "use_hdf = __no_prompt__ or input(\"Do you want to use an HDF file [Y/n]:\") != \"n\" if hdf_path.exists() else False\n",
    "\n",
    "# Read from hdf if availible (much faster)\n",
    "if use_hdf and not __no_hdf__:\n",
    "    df = pd.read_hdf(hdf_path)\n",
    "else:\n",
    "    filepath = data_folder / \"train_users_2.csv\"\n",
    "    dtypes={'id': 'string', 'date_account_created': 'string', 'timestamp_first_active': 'string', 'date_first_booking': 'string', 'gender': 'category', 'age': 'float64', 'signup_method': 'category', 'signup_flow': 'category', 'language': 'category', 'affiliate_channel': 'category', 'affiliate_provider': 'category', 'first_affiliate_tracked': 'category', 'signup_app': 'category', 'first_device_type': 'category', 'first_browser': 'category', 'country_destination': 'category'}\n",
    "    parse_dates = ['date_account_created', 'timestamp_first_active', 'date_first_booking']\n",
    "    cols = list(pd.read_csv(filepath, nrows=1))[1:]\n",
    "    df = pd.read_csv(filepath, dtype=dtypes, na_values=['-unknown-', '<NA>'], parse_dates=parse_dates\n",
    "                     , infer_datetime_format=True) # usecols =[i for i in cols if i != 'id']\n",
    "    df['date_first_booking'] = pd.to_datetime(pd.Series(df['date_first_booking'])\n",
    "                                              , format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "#     df.select_dtypes('datetime64[ns]').fillna(pd.NaT)\n",
    "    df = df.set_index('id')\n",
    "    df['age'] = df['age'].replace(range(2000, 2015), np.nan)\n",
    "    df['age'] = pd.cut(df['age'], bins = [i*5 for i in range(0, 21)] + [120]\n",
    "                       , labels=(['%d-%d' % (i*5, i*5+4) for i in range(0, 20)] + ['100+']))\n",
    "    \n",
    "#     filepath = data_folder / \"age_gender_bkts.csv\"\n",
    "#     dtypes = {}\n",
    "    \n",
    "#     filepath = data_folder / \"sessions.csv\"\n",
    "#     dtypes={'id': 'string', 'action': 'category', 'action_type': 'category', 'action_detail': 'category', 'device_type': 'category', 'sec_elapsed': 'float64'}\n",
    "#     df_session = pd.read_csv(filepath, dtype=dtypes, na_values=['-unknown-', 'NDF', '<NA>'])\n",
    "    \n",
    "    # Save to hdf if reading from csv\n",
    "    df.to_hdf(data_folder / \"data.h5\", key='df', mode='w', format=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b84b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc(lambda x: df['age'] == 1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_session['action'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd313240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_session['action_type'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_session['action_detail'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cat in df_session['action_type'].cat.categories:\n",
    "#     new_column = pd.Series(name=cat, dtype='int8')\n",
    "#     df.insert(new_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc['gxn3p5htnn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe829025",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- - test_users_2.csv - -\")\n",
    "print(\"Number of lines present: \", len(df))\n",
    "print(\"Number of Columns: \", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "topCount = 5\n",
    "print(\"Top \", topCount, \" dataFrames:\")\n",
    "print(df.head(topCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b42d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True, index=False).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True, index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8efc09",
   "metadata": {},
   "source": [
    "### NDCG Implementation\n",
    "\n",
    "(1, 0, 0, 0, 0) will serve as the true ranking.\n",
    "\n",
    "The prediction (NDF-US-OTHER-FR-IT) will be used as a dummy model. It will be transformed into an ndarray such that incorrect guesses are transformed to 0 and correct guesses to 1.\n",
    "\n",
    "A first-rank correct guess generally appears as (1, 0, 0, 0, 0), a second-rank correct guess as (0, 1, 0, 0, 0), and no correct guess as (0, 0, 0, 0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045d1f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dcg(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "def ndcg(r, k):\n",
    "    dcg_max = dcg(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return dcg(r, k) / dcg_max\n",
    "\n",
    "def ndcg_score(y_pred, y_true):\n",
    "    if len(y_pred) != len(y_true):\n",
    "        raise Exception(\"Array lengths do not match: \" + str(y_pred.shape) + \", \" + str(y_true.shape))\n",
    "    rank = []\n",
    "    for i in range(len(y_true)):\n",
    "        y = pd.Series(y_pred[i])\n",
    "        rank.append([y.where(y == y_true[i], other=0).replace(y_true[i], 1)])\n",
    "    rank = np.vstack(rank)\n",
    "    return mean([ndcg(r, len(y_pred[0])) for r in rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bff7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for the dummy model\n",
    "y_pred = [['NDF', 'US', 'other', 'FR', 'IT']]*df.shape[0]\n",
    "\n",
    "ndcg_score(y_pred, df['country_destination'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28563271",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['country_destination'])\n",
    "X = X.drop(columns=['date_account_created', 'timestamp_first_active', 'date_first_booking']) # TODO: Fix datetimes fucking everthing up.\n",
    "y = df['country_destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c8f0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This block can be commented out when doing prediction on the Kaggle test.csv\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.37, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_transformer = make_pipeline(\n",
    "# #     SimpleImputer(strategy='mean'),\n",
    "# #     StandardScaler()\n",
    "# )\n",
    "\n",
    "encoder = make_column_transformer(\n",
    "#     (numeric_transformer, ['age']),\n",
    "    (OneHotEncoder(sparse=False, handle_unknown='ignore'), make_column_selector(dtype_include='category')),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064de3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_enc = LabelEncoder().fit(y_train.values)\n",
    "y_train = y_enc.transform(y_train)\n",
    "y_test = y_enc.transform(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c5ce3",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = LogisticRegression(max_iter=300)\n",
    "\n",
    "pipeline0 = make_pipeline(encoder, clf0)\n",
    "pipeline0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cada3dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline0.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_rank = pipeline0.predict_proba(X_test).argsort()[:, :5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b086a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ndcg_score(y_rank, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290bb7be",
   "metadata": {},
   "source": [
    "#### LR with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a775bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'logisticregression__C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'logisticregression__penalty': ['l2'], # ,l1\n",
    "    'logisticregression__solver': ['lbfgs', 'liblinear', 'saga']\n",
    "#     'columntransformer__pipeline__simpleimputer__strategy': [\"mean\", \"median\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline0, params, n_jobs=-1, verbose=1, cv=5)\n",
    "# pipeline0.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c37f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with parallel_backend(\"ray\"):\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rank = grid_search.predict_proba(X_test).argsort()[:, :5:-1]\n",
    "ndcg_score(y_rank, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19131dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline0, \"models/LogisticRegression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689097e",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53527f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "pipeline1 = make_pipeline(encoder, clf1)\n",
    "pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5463fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "pipeline1.fit(X_train, y_train)\n",
    "\n",
    "# Score the pipeline on the testing data\n",
    "pipeline1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef08e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline1, \"models/CategoricalNB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735429b",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef734b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SVC()\n",
    "\n",
    "pipeline2 = make_pipeline(encoder, clf2)\n",
    "pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Score the pipeline on the testing data\n",
    "pipeline2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ee19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline2, \"models/SVC.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2ba13",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9905d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = MLPClassifier(random_state=1, max_iter=300)\n",
    "\n",
    "pipeline3 = make_pipeline(encoder, clf3)\n",
    "pipeline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba407e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dedfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the pipeline on the testing data\n",
    "pipeline3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725101b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline3, \"models/MLPClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e17495",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "pipeline4 = make_pipeline(encoder, clf4)\n",
    "pipeline4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the pipeline on the testing data\n",
    "pipeline4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline4, \"models/RandomForestClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb9c70",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 =SGDClassifier(random_state=0)\n",
    "\n",
    "pipeline5 = make_pipeline(encoder, clf5)\n",
    "pipeline5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e47eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline5.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de910568",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ''\n",
    "    'sgdclassifier__loss': ['hinge', 'squared_hinge', 'modified_huber', 'perceptron'],\n",
    "    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'sgdclassifier__alpha': [1e-4, 1e-3, 1e-1],\n",
    "    'sgdclassifier__epsilon': [0.01, 0.1]\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline5, params, n_jobs=-1, verbose=1, cv=5)\n",
    "# pipeline0.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with parallel_backend(\"ray\"):\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25888ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rank = grid_search.predict_proba(X_test).argsort()[:, :5:-1]\n",
    "ndcg_score(y_rank, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26403b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline5, \"models/SGDClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71147f",
   "metadata": {},
   "source": [
    "### XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6 =xgb.XGBClassifier(objective='mulit:softprob')\n",
    "\n",
    "pipeline6 = make_pipeline(encoder, clf6)\n",
    "pipeline6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the pipeline on the training data\n",
    "pipeline6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the pipeline on the testing data\n",
    "y_rank = pipeline6.predict_proba(X_test).argsort()[:, :5:-1]\n",
    "ndcg_score(y_rank, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model and write to hard drive\n",
    "# joblib.dump(pipeline6, \"models/XGBClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae7fb00",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf5 = KMeans(n_clusters=3, max_iter=100, random_state=0)\n",
    "# clf5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596803d3",
   "metadata": {},
   "source": [
    "## Predictions for Kaggle's test_users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"airbnb-recruiting-new-user-bookings/test_users_.csv\"\n",
    "# df = pd.read_csv(filepath, dtype=dtypes[:-1], na_values=['-unknown-', 'NDF', '<NA>'], parse_dates=parse_dates, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1743d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = list(df['MachineIdentifier'])\n",
    "# X_kaggle = df.drop(columns=['MachineIdentifier']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef059e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pipeline.transform(X_kaggle.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"submission.csv\", \"w\", newline=\"\") as csvfile:\n",
    "#     csv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"')\n",
    "#     csv_writer.writerows(zip(id_list, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
